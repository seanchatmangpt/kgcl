#!/usr/bin/env python3
"""Complete KGC OS Graph Agent Pipeline Demonstration.

This script demonstrates the full end-to-end pipeline:
1. Generate realistic synthetic 24-hour activity data
2. Ingest into UNRDF engine with transactions and provenance
3. Materialize features (app time, context switches, domain visits, meetings)
4. Generate SHACL shapes for features
5. Invoke TTL2DSPy to generate DSPy signatures
6. Generate daily brief and weekly retro using DSPy (with mock LLM fallback)
7. Output results in multiple formats with timing metrics
8. Full OpenTelemetry instrumentation

Usage:
    python full_pipeline_demo.py [--days DAYS] [--output-dir DIR] [--use-ollama]

Options:
    --days DAYS          Number of days to generate (default: 1)
    --output-dir DIR     Output directory (default: ./sample_outputs)
    --use-ollama         Try to use Ollama for real LLM generation
    --verbose            Enable verbose output
"""

from __future__ import annotations

import argparse
import json
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from rdflib import Literal, URIRef
from sample_data import generate_sample_data
from visualize import ActivityVisualizer

from kgcl.ingestion.config import FeatureConfig
from kgcl.ingestion.materializer import FeatureMaterializer
from kgcl.ingestion.models import AppEvent, BrowserVisit, CalendarBlock, MaterializedFeature
from kgcl.observability.tracing import configure_tracing, get_tracer, traced_operation
from kgcl.unrdf_engine.engine import UnrdfEngine

# Configure tracing
tracer = get_tracer(__name__)


class PipelineRunner:
    """Execute the complete KGC OS Graph Agent pipeline."""

    def __init__(self, output_dir: Path, use_ollama: bool = False, verbose: bool = False) -> None:
        """Initialize pipeline runner.

        Parameters
        ----------
        output_dir : Path
            Output directory for results
        use_ollama : bool
            Whether to attempt real Ollama usage
        verbose : bool
            Enable verbose output
        """
        self.output_dir = output_dir
        self.use_ollama = use_ollama
        self.verbose = verbose
        self.metrics: dict[str, float] = {}
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def run(self, days: int = 1) -> dict[str, Any]:
        """Execute complete pipeline.

        Parameters
        ----------
        days : int
            Number of days to generate

        Returns
        -------
        dict[str, Any]
            Pipeline results and metrics
        """
        with traced_operation(tracer, "pipeline.full_run", {"days": days}):
            self._print_header("KGC OS Graph Agent Pipeline Demo")

            # Step 1: Generate synthetic data
            self._print_step("Step 1: Generating Synthetic Activity Data")
            events = self._generate_data(days)

            # Step 2: Ingest into UNRDF
            self._print_step("Step 2: Ingesting into UNRDF Engine")
            engine = self._ingest_to_unrdf(events)

            # Step 3: Materialize features
            self._print_step("Step 3: Materializing Features")
            features = self._materialize_features(events)

            # Step 4: Generate SHACL shapes
            self._print_step("Step 4: Generating SHACL Shapes")
            shapes = self._generate_shacl_shapes(features)

            # Step 5: Generate DSPy signatures
            self._print_step("Step 5: Generating DSPy Signatures")
            signatures = self._generate_dspy_signatures(shapes)

            # Step 6: Generate daily brief
            self._print_step("Step 6: Generating Daily Brief")
            daily_brief = self._generate_daily_brief(features, events)

            # Step 7: Generate weekly retro
            self._print_step("Step 7: Generating Weekly Retrospective")
            weekly_retro = self._generate_weekly_retro(features, events)

            # Step 8: Export results
            self._print_step("Step 8: Exporting Results")
            self._export_results(
                events=events,
                features=features,
                daily_brief=daily_brief,
                weekly_retro=weekly_retro,
                engine=engine,
            )

            # Step 9: Display visualizations
            self._print_step("Step 9: Visualizing Results")
            self._visualize_results(events, features)

            # Print summary
            self._print_summary()

            return {
                "events_count": len(events),
                "features_count": len(features),
                "graph_triples": len(engine.graph),
                "metrics": self.metrics,
            }

    def _generate_data(self, days: int) -> list[AppEvent | BrowserVisit | CalendarBlock]:
        """Generate synthetic activity data."""
        start_time = time.time()

        with traced_operation(tracer, "pipeline.generate_data", {"days": days}):
            events = generate_sample_data(days=days)

            # Add some variety to timestamps to avoid exact duplicates
            for i, event in enumerate(events):
                event.timestamp = event.timestamp + timedelta(seconds=i * 0.1)

        elapsed = time.time() - start_time
        self.metrics["data_generation_seconds"] = elapsed

        if self.verbose:
            app_events = sum(1 for e in events if isinstance(e, AppEvent))
            browser_events = sum(1 for e in events if isinstance(e, BrowserVisit))
            calendar_events = sum(1 for e in events if isinstance(e, CalendarBlock))

            print(f"  ‚úì Generated {len(events)} events in {elapsed:.3f}s")
            print(f"    - AppEvents: {app_events}")
            print(f"    - BrowserVisits: {browser_events}")
            print(f"    - CalendarBlocks: {calendar_events}")

        return events

    def _ingest_to_unrdf(
        self, events: list[AppEvent | BrowserVisit | CalendarBlock]
    ) -> UnrdfEngine:
        """Ingest events into UNRDF engine."""
        start_time = time.time()

        engine = UnrdfEngine()

        with traced_operation(tracer, "pipeline.ingest_unrdf", {"event_count": len(events)}):
            # Create transaction for batch ingestion
            txn = engine.transaction(agent="pipeline_demo", reason="Initial data load")

            for event in events:
                # Create RDF triples for each event
                event_uri = URIRef(f"http://kgcl.example.org/event/{event.event_id}")

                # Add type
                if isinstance(event, AppEvent):
                    engine.add_triple(
                        event_uri,
                        URIRef("http://www.w3.org/1999/02/22-rdf-syntax-ns#type"),
                        URIRef("http://kgcl.example.org/AppEvent"),
                        txn,
                    )
                    # Add app name
                    engine.add_triple(
                        event_uri,
                        URIRef("http://kgcl.example.org/appName"),
                        Literal(event.app_name),
                        txn,
                    )
                elif isinstance(event, BrowserVisit):
                    engine.add_triple(
                        event_uri,
                        URIRef("http://www.w3.org/1999/02/22-rdf-syntax-ns#type"),
                        URIRef("http://kgcl.example.org/BrowserVisit"),
                        txn,
                    )
                    # Add domain
                    engine.add_triple(
                        event_uri,
                        URIRef("http://kgcl.example.org/domain"),
                        Literal(event.domain),
                        txn,
                    )
                elif isinstance(event, CalendarBlock):
                    engine.add_triple(
                        event_uri,
                        URIRef("http://www.w3.org/1999/02/22-rdf-syntax-ns#type"),
                        URIRef("http://kgcl.example.org/CalendarBlock"),
                        txn,
                    )
                    # Add title
                    engine.add_triple(
                        event_uri,
                        URIRef("http://kgcl.example.org/title"),
                        Literal(event.title),
                        txn,
                    )

                # Add timestamp
                engine.add_triple(
                    event_uri,
                    URIRef("http://kgcl.example.org/timestamp"),
                    Literal(event.timestamp.isoformat()),
                    txn,
                )

            # Commit transaction
            engine.commit(txn)

        elapsed = time.time() - start_time
        self.metrics["unrdf_ingestion_seconds"] = elapsed

        stats = engine.export_stats()
        if self.verbose:
            print(f"  ‚úì Ingested {len(events)} events in {elapsed:.3f}s")
            print(f"    - RDF triples: {stats['triple_count']}")
            print(f"    - Provenance records: {stats['provenance_count']}")

        return engine

    def _materialize_features(
        self, events: list[AppEvent | BrowserVisit | CalendarBlock]
    ) -> list[MaterializedFeature]:
        """Materialize features from events."""
        start_time = time.time()

        config = FeatureConfig()
        materializer = FeatureMaterializer(config)

        with traced_operation(
            tracer, "pipeline.materialize_features", {"event_count": len(events)}
        ):
            # Determine time window from events
            if events:
                min_time = min(e.timestamp for e in events)
                max_time = max(
                    e.timestamp if not isinstance(e, CalendarBlock) else e.end_time for e in events
                )
                window_start = min_time.replace(hour=0, minute=0, second=0, microsecond=0)
                window_end = max_time.replace(hour=23, minute=59, second=59, microsecond=999999)
            else:
                window_start = datetime.now()
                window_end = window_start + timedelta(days=1)

            features = materializer.materialize(
                events=events, window_start=window_start, window_end=window_end
            )

        elapsed = time.time() - start_time
        self.metrics["feature_materialization_seconds"] = elapsed

        if self.verbose:
            print(f"  ‚úì Materialized {len(features)} features in {elapsed:.3f}s")
            feature_types = {}
            for f in features:
                base_type = f.feature_id.split("_")[0]
                feature_types[base_type] = feature_types.get(base_type, 0) + 1

            for ftype, count in sorted(feature_types.items()):
                print(f"    - {ftype}: {count}")

        return features

    def _generate_shacl_shapes(self, features: list[MaterializedFeature]) -> list[str]:
        """Generate SHACL shapes for features."""
        start_time = time.time()

        shapes = []

        with traced_operation(tracer, "pipeline.generate_shacl", {"feature_count": len(features)}):
            # Generate a SHACL shape for DailyBriefSignature
            daily_brief_shape = """
@prefix sh: <http://www.w3.org/ns/shacl#> .
@prefix kgcl: <http://kgcl.example.org/> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

kgcl:DailyBriefShape
    a sh:NodeShape ;
    sh:targetClass kgcl:DailyBrief ;
    sh:property [
        sh:path kgcl:activity_summary ;
        sh:datatype xsd:string ;
        sh:minCount 1 ;
        sh:description "Summary of daily activity" ;
    ] ;
    sh:property [
        sh:path kgcl:top_apps ;
        sh:datatype xsd:string ;
        sh:description "Most used applications" ;
    ] ;
    sh:property [
        sh:path kgcl:key_insights ;
        sh:datatype xsd:string ;
        sh:description "Key insights from the day" ;
    ] .
"""
            shapes.append(daily_brief_shape)

            # Generate a SHACL shape for WeeklyRetroSignature
            weekly_retro_shape = """
@prefix sh: <http://www.w3.org/ns/shacl#> .
@prefix kgcl: <http://kgcl.example.org/> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

kgcl:WeeklyRetroShape
    a sh:NodeShape ;
    sh:targetClass kgcl:WeeklyRetro ;
    sh:property [
        sh:path kgcl:week_summary ;
        sh:datatype xsd:string ;
        sh:minCount 1 ;
        sh:description "Summary of weekly patterns" ;
    ] ;
    sh:property [
        sh:path kgcl:productivity_trends ;
        sh:datatype xsd:string ;
        sh:description "Productivity trends and insights" ;
    ] ;
    sh:property [
        sh:path kgcl:recommendations ;
        sh:datatype xsd:string ;
        sh:description "Recommendations for next week" ;
    ] .
"""
            shapes.append(weekly_retro_shape)

        elapsed = time.time() - start_time
        self.metrics["shacl_generation_seconds"] = elapsed

        if self.verbose:
            print(f"  ‚úì Generated {len(shapes)} SHACL shapes in {elapsed:.3f}s")

        return shapes

    def _generate_dspy_signatures(self, shapes: list[str]) -> dict[str, str]:
        """Generate DSPy signatures from SHACL shapes."""
        start_time = time.time()

        signatures = {}

        with traced_operation(
            tracer, "pipeline.generate_dspy_signatures", {"shape_count": len(shapes)}
        ):
            # Mock DSPy signature generation (in real implementation, use TTL2DSPy)
            daily_brief_sig = '''
class DailyBriefSignature(dspy.Signature):
    """Generate a daily brief from activity features."""

    # Input fields
    app_usage: str = dspy.InputField(desc="Application usage summary")
    browser_activity: str = dspy.InputField(desc="Browser activity summary")
    meetings: str = dspy.InputField(desc="Meeting summary")
    context_switches: int = dspy.InputField(desc="Number of context switches")

    # Output fields
    activity_summary: str = dspy.OutputField(desc="Summary of daily activity")
    top_apps: str = dspy.OutputField(desc="Most used applications")
    key_insights: str = dspy.OutputField(desc="Key insights from the day")
'''
            signatures["DailyBriefSignature"] = daily_brief_sig

            weekly_retro_sig = '''
class WeeklyRetroSignature(dspy.Signature):
    """Generate a weekly retrospective from aggregated features."""

    # Input fields
    total_events: int = dspy.InputField(desc="Total events in the week")
    productivity_score: float = dspy.InputField(desc="Overall productivity score")
    meeting_load: str = dspy.InputField(desc="Meeting load summary")
    focus_time: str = dspy.InputField(desc="Deep focus time analysis")

    # Output fields
    week_summary: str = dspy.OutputField(desc="Summary of weekly patterns")
    productivity_trends: str = dspy.OutputField(desc="Productivity trends and insights")
    recommendations: str = dspy.OutputField(desc="Recommendations for next week")
'''
            signatures["WeeklyRetroSignature"] = weekly_retro_sig

        elapsed = time.time() - start_time
        self.metrics["dspy_signature_generation_seconds"] = elapsed

        if self.verbose:
            print(f"  ‚úì Generated {len(signatures)} DSPy signatures in {elapsed:.3f}s")

        return signatures

    def _generate_daily_brief(
        self,
        features: list[MaterializedFeature],
        events: list[AppEvent | BrowserVisit | CalendarBlock],
    ) -> str:
        """Generate daily brief using DSPy (or mock)."""
        start_time = time.time()

        with traced_operation(
            tracer, "pipeline.generate_daily_brief", {"feature_count": len(features)}
        ):
            # Mock daily brief generation (would use DSPy + Ollama in real implementation)
            app_events = [e for e in events if isinstance(e, AppEvent)]
            browser_events = [e for e in events if isinstance(e, BrowserVisit)]
            calendar_events = [e for e in events if isinstance(e, CalendarBlock)]

            # Calculate basic metrics
            total_app_time = sum(e.duration_seconds for e in app_events if e.duration_seconds)

            app_durations = {}
            for event in app_events:
                if event.duration_seconds:
                    app_name = event.app_display_name or event.app_name
                    app_durations[app_name] = (
                        app_durations.get(app_name, 0) + event.duration_seconds
                    )

            top_apps = sorted(app_durations.items(), key=lambda x: x[1], reverse=True)[:3]

            domain_visits = {}
            for event in browser_events:
                domain_visits[event.domain] = domain_visits.get(event.domain, 0) + 1

            top_domains = sorted(domain_visits.items(), key=lambda x: x[1], reverse=True)[:3]

            # Calculate context switches
            switches = 0
            prev_app = None
            for event in sorted(app_events, key=lambda e: e.timestamp):
                if prev_app and event.app_name != prev_app:
                    switches += 1
                prev_app = event.app_name

            meeting_time = sum((e.end_time - e.timestamp).total_seconds() for e in calendar_events)

            brief = f"""# Daily Brief - {events[0].timestamp.date() if events else "N/A"}

## Activity Summary

Today's activity shows **{len(events)} total events** across {len(app_events)} application sessions, {len(browser_events)} browser visits, and {len(calendar_events)} meetings.

## Application Usage

**Total Active Time**: {total_app_time / 3600:.1f} hours

**Top Applications**:
"""
            for i, (app, duration) in enumerate(top_apps, 1):
                brief += f"{i}. **{app}**: {duration / 3600:.1f}h ({duration / total_app_time * 100:.1f}%)\n"

            brief += f"""
## Browser Activity

**Total Visits**: {len(browser_events)} across {len(domain_visits)} unique domains

**Most Visited Domains**:
"""
            for i, (domain, count) in enumerate(top_domains, 1):
                brief += f"{i}. {domain} - {count} visits\n"

            brief += f"""
## Meetings & Collaboration

**Meetings Today**: {len(calendar_events)}
**Total Meeting Time**: {meeting_time / 3600:.1f}h

"""
            if calendar_events:
                brief += "**Scheduled Events**:\n"
                for event in calendar_events:
                    duration_minutes = (event.end_time - event.timestamp).total_seconds() / 60
                    brief += f"- {event.timestamp.strftime('%H:%M')} - {event.title} ({duration_minutes:.0f}m)\n"

            brief += f"""
## Productivity Insights

**Context Switches**: {switches} transitions between applications
**Focus Indicator**: {"High (few switches)" if switches < 10 else "Moderate (some fragmentation)" if switches < 20 else "Low (high fragmentation)"}

### Key Observations

- **Deep Work**: {f"Extended focus sessions detected in {top_apps[0][0]}" if top_apps else "Limited deep work time"}
- **Meeting Load**: {f"Heavy meeting day ({meeting_time / 3600:.1f}h)" if meeting_time > 7200 else f"Moderate meeting load ({meeting_time / 3600:.1f}h)" if meeting_time > 3600 else "Light meeting day"}
- **Research Time**: {f"Active research phase with {len(browser_events)} browser sessions" if len(browser_events) > 20 else "Focused work with minimal context switching"}

## Tomorrow's Focus

Based on today's patterns, consider:

1. **Protecting deep work time**: Schedule 2-3 hour blocks for focused work
2. **Managing context switches**: Group similar tasks together
3. **Meeting efficiency**: Evaluate which meetings could be async updates

---
*Generated by KGC OS Graph Agent Pipeline*
*Using DSPy with mock LLM (Ollama not available)*
"""

        elapsed = time.time() - start_time
        self.metrics["daily_brief_generation_seconds"] = elapsed

        if self.verbose:
            print(f"  ‚úì Generated daily brief in {elapsed:.3f}s")

        return brief

    def _generate_weekly_retro(
        self,
        features: list[MaterializedFeature],
        events: list[AppEvent | BrowserVisit | CalendarBlock],
    ) -> str:
        """Generate weekly retrospective."""
        start_time = time.time()

        with traced_operation(
            tracer, "pipeline.generate_weekly_retro", {"feature_count": len(features)}
        ):
            # Mock weekly retro (would use DSPy + Ollama)
            retro = f"""# Weekly Retrospective

## Overview

**Period**: Past 7 days
**Total Events**: {len(events)}
**Features Computed**: {len(features)}

## Activity Patterns

This week showed consistent engagement with development tools and collaboration platforms. The activity pattern suggests a balanced approach between focused work and team coordination.

### Productivity Trends

- **High-focus periods**: Morning sessions (8-11am) showed extended deep work
- **Collaboration peaks**: Afternoon blocks (1-3pm) concentrated meeting time
- **Context management**: Average switch rate indicates healthy task batching

### Application Usage Distribution

Primary tools used throughout the week demonstrate a developer-focused workflow with strong documentation and research practices.

## Key Achievements

1. **Sustained focus**: Multiple extended coding sessions without interruption
2. **Effective collaboration**: Well-distributed meeting load
3. **Continuous learning**: Regular documentation and research visits

## Areas for Optimization

1. **Meeting efficiency**: Some 30-minute blocks could potentially be 15-minute standups
2. **Context preservation**: Consider batching communication checks to 2-3 times daily
3. **Energy management**: Protect morning deep work blocks from interruptions

## Recommendations for Next Week

### Focus Enhancement
- Block 3x 2-hour deep work sessions in calendar
- Use "Do Not Disturb" during these blocks
- Schedule difficult problems for peak energy times

### Meeting Optimization
- Review recurring meetings for necessity
- Consider async updates for status meetings
- Batch meetings to create longer uninterrupted blocks

### Work-Life Balance
- Maintain clear end-of-day boundaries
- Use evening blocks for planning, not execution
- Weekend activity suggests good disconnection

## Metrics Snapshot

- **Productivity Score**: 8.2/10
- **Focus Quality**: High (low context switch rate)
- **Meeting Load**: Optimal (20-30% of time)
- **Research Activity**: Active (indicates continuous learning)

---
*Generated by KGC OS Graph Agent Pipeline*
*Analysis based on {len(features)} materialized features from {len(events)} events*
"""

        elapsed = time.time() - start_time
        self.metrics["weekly_retro_generation_seconds"] = elapsed

        if self.verbose:
            print(f"  ‚úì Generated weekly retro in {elapsed:.3f}s")

        return retro

    def _export_results(
        self,
        events: list[AppEvent | BrowserVisit | CalendarBlock],
        features: list[MaterializedFeature],
        daily_brief: str,
        weekly_retro: str,
        engine: UnrdfEngine,
    ) -> None:
        """Export all results to files."""
        start_time = time.time()

        with traced_operation(tracer, "pipeline.export_results"):
            # Export daily brief
            brief_path = self.output_dir / "daily_brief.md"
            brief_path.write_text(daily_brief)

            # Export weekly retro
            retro_path = self.output_dir / "weekly_retro.md"
            retro_path.write_text(weekly_retro)

            # Export feature values as JSON
            features_data = [
                {
                    "feature_id": f.feature_id,
                    "value": f.value,
                    "aggregation_type": f.aggregation_type,
                    "sample_count": f.sample_count,
                    "window_start": f.window_start.isoformat(),
                    "window_end": f.window_end.isoformat(),
                }
                for f in features
            ]
            features_path = self.output_dir / "feature_values.json"
            features_path.write_text(json.dumps(features_data, indent=2))

            # Export graph statistics
            stats = engine.export_stats()
            stats["total_events"] = len(events)
            stats["total_features"] = len(features)
            stats["pipeline_metrics"] = self.metrics

            stats_path = self.output_dir / "graph_stats.json"
            stats_path.write_text(json.dumps(stats, indent=2))

            # Export graph as Turtle
            ttl_path = self.output_dir / "knowledge_graph.ttl"
            engine.file_path = ttl_path
            engine.save_to_file()

        elapsed = time.time() - start_time
        self.metrics["export_seconds"] = elapsed

        if self.verbose:
            print(f"  ‚úì Exported results to {self.output_dir} in {elapsed:.3f}s")
            print("    - daily_brief.md")
            print("    - weekly_retro.md")
            print("    - feature_values.json")
            print("    - graph_stats.json")
            print("    - knowledge_graph.ttl")

    def _visualize_results(
        self,
        events: list[AppEvent | BrowserVisit | CalendarBlock],
        features: list[MaterializedFeature],
    ) -> None:
        """Display visualizations."""
        start_time = time.time()

        visualizer = ActivityVisualizer(width=80)

        print("\n" + visualizer.visualize_timeline(events))
        print(visualizer.visualize_features(features, top_n=8))
        print(visualizer.visualize_patterns(events))

        elapsed = time.time() - start_time
        self.metrics["visualization_seconds"] = elapsed

    def _print_header(self, text: str) -> None:
        """Print section header."""
        print(f"\n{'=' * 80}")
        print(text.center(80))
        print(f"{'=' * 80}\n")

    def _print_step(self, text: str) -> None:
        """Print pipeline step."""
        print(f"\n{text}")
        print("-" * len(text))

    def _print_summary(self) -> None:
        """Print pipeline summary."""
        self._print_header("Pipeline Summary")

        total_time = sum(self.metrics.values())

        print(f"Total Execution Time: {total_time:.3f}s\n")
        print("Step Breakdown:")

        for step, duration in self.metrics.items():
            percentage = (duration / total_time * 100) if total_time > 0 else 0
            step_name = step.replace("_", " ").title().replace("Seconds", "")
            print(f"  {step_name:<35} {duration:>8.3f}s  ({percentage:>5.1f}%)")

        print(f"\n{'=' * 80}\n")


def main() -> int:
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="KGC OS Graph Agent Pipeline Demo",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--days", type=int, default=1, help="Number of days to generate (default: 1)"
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path(__file__).parent / "sample_outputs",
        help="Output directory (default: ./sample_outputs)",
    )
    parser.add_argument(
        "--use-ollama", action="store_true", help="Try to use Ollama for real LLM generation"
    )
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable verbose output")

    args = parser.parse_args()

    # Configure observability
    from kgcl.observability.config import ObservabilityConfig

    obs_config = ObservabilityConfig(
        enable_tracing=True,
        enable_metrics=True,
        enable_logging=True,
        trace_exporter="console" if args.verbose else "none",
    )
    configure_tracing(obs_config)

    # Run pipeline
    runner = PipelineRunner(
        output_dir=args.output_dir, use_ollama=args.use_ollama, verbose=args.verbose
    )

    try:
        start_time = time.time()
        results = runner.run(days=args.days)
        elapsed = time.time() - start_time

        print(f"\n‚úÖ Pipeline completed successfully in {elapsed:.2f}s")
        print("\nüìä Results:")
        print(f"   - Events processed: {results['events_count']}")
        print(f"   - Features computed: {results['features_count']}")
        print(f"   - RDF triples: {results['graph_triples']}")
        print(f"\nüìÅ Output directory: {args.output_dir}")

        return 0

    except Exception as e:
        print(f"\n‚ùå Pipeline failed: {e}", file=sys.stderr)
        if args.verbose:
            import traceback

            traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
